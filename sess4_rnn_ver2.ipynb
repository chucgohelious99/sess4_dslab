{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sess4_rnn_ver2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnrgX_dmBCoo",
        "colab_type": "code",
        "outputId": "916f6e2a-9e14-4859-8da1-ffe0d7e06496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp5ltJ00BZqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP90hovtX1pR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import  isfile\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "MAX_DOC_LENGTH=500\n",
        "unknown_ID=-999\n",
        "padding_ID=-1000\n",
        "\n",
        "def gen_data_and_vocab():\n",
        "    def collect_data_from(parent_path, newsgroup_list, word_count=None):\n",
        "        data=[]\n",
        "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
        "            dir_path= parent_path + '/' + newsgroup+'/'\n",
        "            files= [(filename, dir_path+ filename)\n",
        "                    for filename in listdir(dir_path)\n",
        "                    if isfile(dir_path+ filename)]\n",
        "            files.sort()\n",
        "            label= group_id\n",
        "            print('processing: {} - {}'.format(group_id, newsgroup))\n",
        "\n",
        "            for filename, file_path in files:\n",
        "                with open(file_path,'rb') as f:\n",
        "                    text= f.read().decode('UTF-8', errors='ignore').lower()\n",
        "                    words= re.split('\\W+', text)\n",
        "                    if word_count is not None: # chi ap dung cho train data\n",
        "                        for word in words:\n",
        "                            word_count[word]+=1\n",
        "                    content= ' '.join(words)\n",
        "                    assert  len(content.splitlines())==1\n",
        "                    data.append(str(label)+ '<fff>'\n",
        "                                + filename+ '<fff>'+ content)\n",
        "        return data\n",
        "\n",
        "    word_count= defaultdict(int)\n",
        "\n",
        "    path= '/content/drive/My Drive/Python code/DS_Lab2/Data/'\n",
        "    parts=[path + dir_name + '/' for dir_name in  listdir(path)\n",
        "           if not isfile(path + dir_name)]\n",
        "\n",
        "    train_path, test_path= (parts[0], parts[1]) if 'train' in parts[0]\\\n",
        "        else (parts[1], parts[0])\n",
        "\n",
        "    newsgroups_list= [newsgroup  for newsgroup in listdir(train_path)]\n",
        "    newsgroups_list.sort()\n",
        "\n",
        "    #xay dung tu dien va thu thap du lieu\n",
        "    train_data= collect_data_from(\n",
        "        parent_path= train_path,\n",
        "        newsgroup_list= newsgroups_list,\n",
        "        word_count= word_count\n",
        "    )\n",
        "    vocab=[word for word, freq in\n",
        "           zip(word_count.keys(), word_count.values())\n",
        "           if freq > 10]\n",
        "    vocab.sort()\n",
        "    with open('/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/vocab-raw.txt','w') as f:\n",
        "        f.write('\\n'.join(vocab))\n",
        "\n",
        "    test_data = collect_data_from(\n",
        "        parent_path=test_path,\n",
        "        newsgroup_list=newsgroups_list\n",
        "    )\n",
        "\n",
        "    with open('/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/20news-train-raw.txt','w') as f:\n",
        "         f.write('\\n'.join(train_data))\n",
        "    with open('/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/20news-test-raw.txt','w') as f:\n",
        "         f.write('\\n'.join(test_data))\n",
        "\n",
        "\n",
        "def encode_data(data_path, vocab_path):\n",
        "    with open(vocab_path) as f:\n",
        "        vocab= dict([(word, word_ID+2)\n",
        "                     for word_ID, word in enumerate(f.read().splitlines())])\n",
        "    with open(data_path) as f:\n",
        "        documents= [(line.split('<fff>')[0],\n",
        "                     line.split('<fff>')[1],\n",
        "                     line.split('<fff>')[2])\n",
        "                    for line in f.read().splitlines()]\n",
        "    \n",
        "    encoded_data= []\n",
        "    for docs in documents:\n",
        "        label, doc_id, text= docs\n",
        "        words= text.split()[:MAX_DOC_LENGTH]\n",
        "        sentence_length= len(words)\n",
        "        \n",
        "        encode_text= []\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                encode_text.append(str(vocab[word]))\n",
        "            else:\n",
        "                encode_text.append((str(unknown_ID)))\n",
        "        if len(words) <MAX_DOC_LENGTH:\n",
        "            num_padding= MAX_DOC_LENGTH - len(words)\n",
        "            for i in range(num_padding):\n",
        "                encode_text.append(str(padding_ID))\n",
        "                \n",
        "        encoded_data.append(str(label)+ '<fff>' +\n",
        "                           str(doc_id) +'<fff>'+\n",
        "                           str(sentence_length)+ '<fff>'+\n",
        "                           ' '.join(encode_text))\n",
        "\n",
        "    dir_name= '/'.join(data_path.split('/')[:-1])\n",
        "    file_name= '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
        "    with open(dir_name +'/'+ file_name, 'w') as f:\n",
        "        f.write('\\n'.join(encoded_data))\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "    gen_data_and_vocab()\n",
        "    train_data_path='/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/20news-train-raw.txt'\n",
        "    test_data_path='/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/20news-test-raw.txt'\n",
        "    vocab_path='/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/vocab-raw.txt'\n",
        "    # encode_data(train_data_path, vocab_path)\n",
        "    encode_data(test_data_path, vocab_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC8SSC1GEMBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/chuc2.txt') as f:\n",
        "    a= f.read()\n",
        "    a=a.replace(\"-999\",\"0\")\n",
        "    a=a.replace(\"-1000\",\"1\")\n",
        "\n",
        "with open('/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/chuc2.txt','w') as f:\n",
        "    f.write(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSU8UcFYFsM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file='/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/20news-test-encoded.txt'\n",
        "with open(file) as f:\n",
        "    a= f.read()\n",
        "    a=a.replace(\"-999\",\"0\")\n",
        "    a=a.replace(\"-1000\",\"1\")\n",
        "\n",
        "with open(file,'w') as f:\n",
        "    f.write(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3QslIv1FscO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file='/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/20news-train-encoded.txt'\n",
        "with open(file) as f:\n",
        "    a= f.read()\n",
        "    a=a.replace(\"-999\",\"0\")\n",
        "    a=a.replace(\"-1000\",\"1\")\n",
        "\n",
        "with open(file,'w') as f:\n",
        "    f.write(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIoK6e9nEAUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow.compat.v1 as tf2\n",
        "import numpy as np\n",
        "import random \n",
        "\n",
        "\n",
        "class DataReader:\n",
        "    def __init__(self, data_path, batch_size, vocab_size):\n",
        "        self._batch_size= batch_size\n",
        "        with open(data_path) as f:\n",
        "            d_lines= f.read().splitlines()\n",
        "        \n",
        "        self._data=[]\n",
        "        self._labels=[]\n",
        "        self._sentence_lengths=[]\n",
        "        for data_id, line in enumerate(d_lines):\n",
        "            # tfidf_vec= [0.0 for i in range(vocab_size)]\n",
        "            if len(line) > 1:\n",
        "                feature= line.split('<fff>')\n",
        "                # print(\"*\"+feature[2] +\"*\")\n",
        "                # print(\"*\"+feature[1] +\"*\")\n",
        "                # print(\"*\"+feature[0] +\"*\")\n",
        "                label, doc_id, sentence_length= int(feature[0]), int(feature[1]), int(feature[2])\n",
        "                # tokens= feature[3].split()\n",
        "                tokens= feature[3].split()\n",
        "                vector= [int(token) for token in tokens]\n",
        "                # for token in tokens:\n",
        "                    # print(\"tên file:\" ,data_path)\n",
        "                    # print(\"dòng thứ: \", data_id)\n",
        "                    # print(\"tokens:\", tokens[:10])\n",
        "                    # print(token)\n",
        "                    # ind, value= int(token.split(\":\")[0]), \\\n",
        "                    #             float(token.split(\":\")[1])\n",
        "                    # tfidf_vec[ind]= value\n",
        "                    \n",
        "                \n",
        "                # self._data.append(tfidf_vec)\n",
        "                self._data.append(vector)\n",
        "                self._labels.append(label)\n",
        "                self._sentence_lengths.append(sentence_length)\n",
        "\n",
        "        self._data= np.array(self._data)\n",
        "        self._labels= np.array(self._labels)\n",
        "        self._sentence_lengths= np.array(self._sentence_lengths)\n",
        "\n",
        "        self._num_epoch=0\n",
        "        self._batch_id=0\n",
        "\n",
        "    def next_batch(self):\n",
        "        #lấy data từ trong batch tiếp theo và xáo chộn lên\n",
        "        start= self._batch_id * self._batch_size\n",
        "        end= start + self._batch_size\n",
        "        self._batch_id +=1\n",
        "\n",
        "        if end + self._batch_size > len(self._data):\n",
        "        # khi đến batch cuối, một epoch có kích thước bằng data, nên \n",
        "        # khi chọn data set cho từng patch cần xáo chộn ngẫu nhiên rồi chọn cho đủ patch\n",
        "        # dù cho trong 1 epoch có thể không chứa toàn bộ dữ liệu nhưng model có thể học dần \n",
        "        #dần thông qua các epoch\n",
        "            start = len(self._data) - self._batch_size\n",
        "            end= len(self._data)\n",
        "            self._num_epoch+=1 # chuyển sang epoch mới\n",
        "            self._batch_id=0\n",
        "        \n",
        "            # xáo trộn data\n",
        "            indices= list(range(len(self._data)))\n",
        "            random.seed(2020)\n",
        "            random.shuffle(indices)\n",
        "            self._data, self._labels,self._sentence_lengths = self._data[indices], self._labels[indices], self._sentence_lengths[indices]\n",
        "\n",
        "        return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y842rZ5CDPi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import  numpy as np\n",
        "MAX_DOC_LENGTH=500\n",
        "NUM_CLASSES=20\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "class RNN :\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "        self._vocab_size= vocab_size\n",
        "        self._embedding_size= embedding_size\n",
        "        self._lstm_size= lstm_size\n",
        "        self._batch_size= batch_size\n",
        "\n",
        "        self._data= tf.placeholder(tf.int32, shape=[batch_size,MAX_DOC_LENGTH])\n",
        "        self._labels= tf.placeholder(tf.int32, shape=[batch_size,])\n",
        "        self._sentence_lengths= tf.placeholder(tf.int32, shape=[batch_size,])\n",
        "        # self._final_tokens= tf.placeholder(tf.int32, shape=[batch_size,])\n",
        "\n",
        "    def embedding_layer(self, indices):\n",
        "        pretrain_vectors= []\n",
        "        pretrain_vectors.append(np.zeros(self._embedding_size))\n",
        "        np.random.seed(2020)\n",
        "        for i in range(self._vocab_size +1):\n",
        "            pretrain_vectors.append(np.random.normal(\n",
        "                loc=0.,\n",
        "                scale=1.,\n",
        "                size= self._embedding_size\n",
        "            ))\n",
        "        pretrain_vectors= np.array(pretrain_vectors)\n",
        "        with tf.variable_scope(\"rnn_variables\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            self._embedding_matrix= tf.get_variable(\n",
        "                name='embedding',\n",
        "                shape=(self._vocab_size +2,self._embedding_size ),\n",
        "                initializer= tf.constant_initializer(pretrain_vectors)\n",
        "            )\n",
        "        return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "    def LSTM_layer(self, embeddings):\n",
        "        lstm_cell= tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
        "        zero_state= tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
        "        initial_state= tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "        lstm_inputs= tf.unstack(\n",
        "            tf.transpose(embeddings, perm=[1,0,2])\n",
        "        )\n",
        "        lstm_outputs, last_state= tf.nn.static_rnn(\n",
        "            cell= lstm_cell,\n",
        "            inputs= lstm_inputs,\n",
        "            initial_state= initial_state,\n",
        "            sequence_length= self._sentence_lengths\n",
        "        ) #danh sách gồ 500 phần tử có dạng [num_doc, lstm_size]\n",
        "\n",
        "        lstm_outputs= tf.unstack(\n",
        "            tf.transpose(lstm_outputs, perm=[1,0,2])\n",
        "        )\n",
        "        lstm_outputs = tf.concat(lstm_outputs, axis=0)\n",
        "\n",
        "        mask= tf.sequence_mask(\n",
        "            lengths=self._sentence_lengths,\n",
        "            maxlen=MAX_DOC_LENGTH,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        mask=tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
        "        mask= tf.expand_dims(mask, -1)\n",
        "\n",
        "        lstm_outputs= mask*lstm_outputs\n",
        "        lstm_outputs_splits=tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n",
        "        lstm_outputs_sum=tf.reduce_sum(lstm_outputs_splits, axis=1)\n",
        "        lstm_outputs_average= lstm_outputs_sum/ tf.expand_dims(\n",
        "            tf.cast(self._sentence_lengths, tf.float32), -1\n",
        "        )\n",
        "\n",
        "        return lstm_outputs_average\n",
        "    def build_graph(self):\n",
        "        embeddings= self.embedding_layer(self._data)\n",
        "        lstm_outputs= self.LSTM_layer(embeddings)\n",
        "\n",
        "        with tf.variable_scope(\"rnn_variables\", reuse= tf.AUTO_REUSE) as scope:\n",
        "            weights= tf.get_variable(\n",
        "                name=\"final_layer_weights\",\n",
        "                shape=(self._lstm_size, NUM_CLASSES),\n",
        "                initializer=tf.random_normal_initializer(seed=2020)\n",
        "            )\n",
        "            biases = tf.get_variable(\n",
        "                name=\"final_layer_biases\",\n",
        "                shape=(NUM_CLASSES),\n",
        "                initializer=tf.random_normal_initializer(seed=2020)\n",
        "            )\n",
        "            logits= tf.matmul(lstm_outputs, weights) + biases\n",
        "\n",
        "            label_one_hot= tf.one_hot(\n",
        "                indices= self._labels,\n",
        "                depth= NUM_CLASSES,\n",
        "                dtype= tf.float32\n",
        "            )\n",
        "\n",
        "            loss= tf.nn.softmax_cross_entropy_with_logits(\n",
        "                labels= label_one_hot,\n",
        "                logits= logits\n",
        "            )\n",
        "\n",
        "            loss= tf.reduce_mean(loss)\n",
        "\n",
        "            probs= tf.nn.softmax(logits)\n",
        "            predicted_labels= tf.argmax(probs, axis=1)\n",
        "            predicted_labels= tf.squeeze(predicted_labels)\n",
        "\n",
        "        return  predicted_labels, loss\n",
        "\n",
        "\n",
        "    def trainer(self,loss, learning_rate):\n",
        "        with tf.variable_scope(\"rnn_variables\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            train_op= tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "        return train_op\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GuywUCeQpfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7cffa07-4d6a-4330-ea64-038dfc7c116f"
      },
      "source": [
        "def train_and_evaluate_RNN():\n",
        "    with open('/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/vocab-raw.txt') as f:\n",
        "        vocab_size= len(f.readlines())\n",
        "\n",
        "    tf.set_random_seed(2020)\n",
        "    rnn= RNN(\n",
        "        vocab_size= vocab_size,\n",
        "        embedding_size= 300,\n",
        "        lstm_size= 50,\n",
        "        batch_size=50\n",
        "    )\n",
        "    predicted_labels, loss= rnn.build_graph()\n",
        "    train_op=rnn.trainer(loss=loss, learning_rate=0.01)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        train_data_reader=DataReader(\n",
        "            data_path='/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/20news-train-encoded.txt',\n",
        "            batch_size=50,\n",
        "            vocab_size= vocab_size\n",
        "        )\n",
        "        test_data_reader= DataReader(\n",
        "            data_path='/content/drive/My Drive/Python code/DS_Lab2/Data/w2v/20news-test-encoded.txt',\n",
        "            batch_size=50,\n",
        "            vocab_size= vocab_size\n",
        "        )\n",
        "        step, MAX_STEP= 0,1500\n",
        "        \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        while step < MAX_STEP:\n",
        "            next_train_batch= train_data_reader.next_batch()\n",
        "            # train_data, train_labels, train_sentence_lengths, train_final_tokens= next_train_batch\n",
        "            train_data, train_labels, train_sentence_lengths= next_train_batch\n",
        "            plabel_eval, loss_eval, _= sess.run(\n",
        "                [predicted_labels, loss, train_op],\n",
        "                feed_dict={\n",
        "                    rnn._data : train_data,\n",
        "                    rnn._labels: train_labels,\n",
        "                    rnn._sentence_lengths: train_sentence_lengths,\n",
        "                    # rnn._final_tokens: train_final_tokens            \n",
        "                }\n",
        "            )\n",
        "            step+=1\n",
        "            if step%20==0:\n",
        "                print(\"step: \"+ str(step)+\" loss: \"+ str(loss_eval))\n",
        "            \n",
        "            if train_data_reader._batch_id==0: #khi vua xet xong 1 epoch -> danh gia tren test data\n",
        "                nums_true_pred=0\n",
        "                while True:\n",
        "                    next_test_batch= test_data_reader.next_batch()\n",
        "                    # test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n",
        "                    test_data, test_labels, test_sentence_lengths = next_test_batch\n",
        "                    test_plabel_eval, loss_eval, _ = sess.run(\n",
        "                        [predicted_labels, loss, train_op],\n",
        "                        feed_dict={\n",
        "                            rnn._data: test_data,\n",
        "                            rnn._labels: test_labels,\n",
        "                            rnn._sentence_lengths: test_sentence_lengths,\n",
        "                            # rnn._final_tokens: test_final_tokens\n",
        "                        }\n",
        "                    )\n",
        "                    matches= np.equal(test_plabel_eval, test_labels)\n",
        "                    nums_true_pred += np.sum(matches.astype(float))\n",
        "                    if test_data_reader._batch_id==0:\n",
        "                        break\n",
        "                print(\"Epoch: \",train_data_reader._num_epoch)\n",
        "                print(\"Accuracy on test data: \", nums_true_pred * 100./len(test_data_reader._data))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    with tf.variable_scope(\"rnn_variables\", reuse= tf.AUTO_REUSE) as scope:\n",
        "        train_and_evaluate_RNN()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From <ipython-input-2-04ccb5e17c83>:39: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-2-04ccb5e17c83>:50: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:740: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-2-04ccb5e17c83>:99: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "step: 20 loss: 0.0002571087\n",
            "step: 40 loss: 1.0598379\n",
            "step: 60 loss: 7.5499415\n",
            "step: 80 loss: 0.45277742\n",
            "step: 100 loss: 1.4315\n",
            "step: 120 loss: 5.4647174\n",
            "step: 140 loss: 1.2392772\n",
            "step: 160 loss: 4.03423\n",
            "step: 180 loss: 4.7991905\n",
            "step: 200 loss: 5.4172707\n",
            "step: 220 loss: 4.50836\n",
            "Epoch:  1\n",
            "Accuracy on test data:  4.567180031864047\n",
            "step: 240 loss: 2.482046\n",
            "step: 260 loss: 2.3428414\n",
            "step: 280 loss: 2.29398\n",
            "step: 300 loss: 1.9501145\n",
            "step: 320 loss: 1.7891027\n",
            "step: 340 loss: 1.8141888\n",
            "step: 360 loss: 1.5753573\n",
            "step: 380 loss: 1.577131\n",
            "step: 400 loss: 1.357904\n",
            "step: 420 loss: 1.2327027\n",
            "step: 440 loss: 1.3112712\n",
            "Epoch:  2\n",
            "Accuracy on test data:  83.64312267657992\n",
            "step: 460 loss: 0.85306954\n",
            "step: 480 loss: 0.68014246\n",
            "step: 500 loss: 0.5038792\n",
            "step: 520 loss: 0.70701575\n",
            "step: 540 loss: 0.7343758\n",
            "step: 560 loss: 0.6246736\n",
            "step: 580 loss: 0.6518765\n",
            "step: 600 loss: 0.4267859\n",
            "step: 620 loss: 0.35936782\n",
            "step: 640 loss: 0.5580307\n",
            "step: 660 loss: 0.46075967\n",
            "Epoch:  3\n",
            "Accuracy on test data:  91.42326075411577\n",
            "step: 680 loss: 0.20292307\n",
            "step: 700 loss: 0.22188652\n",
            "step: 720 loss: 0.14198539\n",
            "step: 740 loss: 0.28253224\n",
            "step: 760 loss: 0.15919809\n",
            "step: 780 loss: 0.18016651\n",
            "step: 800 loss: 0.17074552\n",
            "step: 820 loss: 0.20118248\n",
            "step: 840 loss: 0.24632102\n",
            "step: 860 loss: 0.14187202\n",
            "step: 880 loss: 0.21000217\n",
            "step: 900 loss: 0.26246095\n",
            "Epoch:  4\n",
            "Accuracy on test data:  97.2251725969198\n",
            "step: 920 loss: 0.03443316\n",
            "step: 940 loss: 0.06856236\n",
            "step: 960 loss: 0.03870354\n",
            "step: 980 loss: 0.033446673\n",
            "step: 1000 loss: 0.06354501\n",
            "step: 1020 loss: 0.048481908\n",
            "step: 1040 loss: 0.078460924\n",
            "step: 1060 loss: 0.04454019\n",
            "step: 1080 loss: 0.17195202\n",
            "step: 1100 loss: 0.023649264\n",
            "step: 1120 loss: 0.04743208\n",
            "Epoch:  5\n",
            "Accuracy on test data:  99.15029208709507\n",
            "step: 1140 loss: 0.012698684\n",
            "step: 1160 loss: 0.01035149\n",
            "step: 1180 loss: 0.022061203\n",
            "step: 1200 loss: 0.0087961815\n",
            "step: 1220 loss: 0.01654439\n",
            "step: 1240 loss: 0.00860907\n",
            "step: 1260 loss: 0.025001217\n",
            "step: 1280 loss: 0.0084876735\n",
            "step: 1300 loss: 0.012895229\n",
            "step: 1320 loss: 0.011541369\n",
            "step: 1340 loss: 0.013037214\n",
            "Epoch:  6\n",
            "Accuracy on test data:  99.46893255443442\n",
            "step: 1360 loss: 0.0040495605\n",
            "step: 1380 loss: 0.0075064288\n",
            "step: 1400 loss: 0.007386604\n",
            "step: 1420 loss: 0.00808192\n",
            "step: 1440 loss: 0.0042478424\n",
            "step: 1460 loss: 0.0038593605\n",
            "step: 1480 loss: 0.005062742\n",
            "step: 1500 loss: 0.004552547\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}